ReLU
ReLU
ReLU
softmax



!---------------------------------------------------------------------------!
!*  每层对应的激活函数：                                                   *!
!*		sigmoid tanh ReLU PReLU ELU linear softmax.                        *!
!*		应与 NNHiddenLayerNodeCount.parameter 文件中参数数量、             *!
!*		     NNLearningRate.parameter 文件中参数数量匹配.                  *!
!---------------------------------------------------------------------------!